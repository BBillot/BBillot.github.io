---
---

@string{aps = {American Physical Society,}}

@article{billot_robust_2023,
    title = {{Robust} machine learning segmentation for large-scale analysis of heterogeneous clinical brain {MRI} datasets},
    author = {Billot, Benjamin and Colin, Magdamo and Cheng, You and Das, Sudeshna and Iglesias, Juan Eugenio},
    journal = {{Proceedings} of the {National} {Academy} of {Sciences} ({PNAS})},
    year = {2023},
    volume = {120},
    number = {9},
    abstract = {Every year, millions of brain MRI scans are acquired in hospitals, which is a figure considerably larger than the size of any research dataset. Therefore, the ability to analyze such scans could transform neuroimaging research. Yet, their potential remains untapped since no automated algorithm is robust enough to cope with the high variability in clinical acquisitions (MR contrasts, resolutions, orientations, artifacts, and subject populations). Here, we present SynthSeg+, an AI segmentation suite that enables robust analysis of heterogeneous clinical datasets. In addition to whole-brain segmentation, SynthSeg+ also performs cortical parcellation, intracranial volume estimation, and automated detection of faulty segmentations (mainly caused by scans of very low quality). We demonstrate SynthSeg+ in seven experiments, including an aging study on 14,000 scans, where it accurately replicates atrophy patterns observed on data of much higher quality. SynthSeg+ is publicly released as a ready-to-use tool to unlock the potential of quantitative morphometry.},
    html={https://www.pnas.org/doi/abs/10.1073/pnas.2216399120},
    pdf={https://www.pnas.org/doi/epdf/10.1073/pnas.2216399120},
    bibtex_show={true},
    abbr={Journal},
    selected={true},
}

@article{iglesias_synthsr_2023,
    title = {SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry},
    author = {Juan E. Iglesias  and Benjamin Billot  and Yaël Balbastre  and Colin Magdamo  and Steven E. Arnold  and Sudeshna Das  and Brian L. Edlow  and Daniel C. Alexander  and Polina Golland  and Bruce Fischl },
    journal = {Science Advances},
    volume = {9},
    number = {5},
    pages = {eadd3607},
    year = {2023},
    abstract = {Every year, millions of brain magnetic resonance imaging (MRI) scans are acquired in hospitals across the world. These have the potential to revolutionize our understanding of many neurological diseases, but their morphometric analysis has not yet been possible due to their anisotropic resolution. We present an artificial intelligence technique, “SynthSR,” that takes clinical brain MRI scans with any MR contrast (T1, T2, etc.), orientation (axial/coronal/sagittal), and resolution and turns them into high-resolution T1 scans that are usable by virtually all existing human neuroimaging tools. We present results on segmentation, registration, and atlasing of \&gt;10,000 scans of controls and patients with brain tumors, strokes, and Alzheimer’s disease. SynthSR yields morphometric results that are very highly correlated with what one would have obtained with high-resolution T1 scans. SynthSR allows sample sizes that have the potential to overcome the power limitations of prospective research studies and shed new light on the healthy and diseased human brain. A public AI tool turns clinical brain MRI of any resolution/contrast into 1mm T1 scans compatible with neuroimaging pipelines.},
    html = {https://www.science.org/doi/abs/10.1126/sciadv.add3607},
    pdf={https://www.science.org/doi/epdf/10.1126/sciadv.add3607},
    bibtex_show={true},
    abbr={Journal},
}

@article{billot_synthseg_2023,
    title = {SynthSeg: {Segmentation} of brain {MRI} scans of any contrast and resolution without retraining},
    author = {Billot, Benjamin and Greve, Douglas N. and Puonti, Oula and Thielscher, Axel and Van Leemput, Koen and Fischl, Bruce and Dalca, Adrian V. and Iglesias, Juan Eugenio},
    journal = {{Medical} {Image} {Analysis}},
    year = {2023},
    volume = {86},
    abstract = {Despite advances in data augmentation and transfer learning, convolutional neural networks (CNNs) difficultly generalise to unseen domains. When segmenting brain scans, CNNs are highly sensitive to changes in resolution and contrast: even within the same MRI modality, performance can decrease across datasets. Here we introduce SynthSeg, the first segmentation CNN robust against changes in contrast and resolution. SynthSeg is trained with synthetic data sampled from a generative model conditioned on segmentations. Crucially, we adopt a domain randomisation strategy where we fully randomise the contrast and resolution of the synthetic training data. Consequently, SynthSeg can segment real scans from a wide range of target domains without retraining or fine-tuning, which enables straightforward analysis of huge amounts of heterogeneous clinical data. Because SynthSeg only requires segmentations to be trained (no images), it can learn from labels obtained by automated methods on diverse populations (e.g., ageing and diseased), thus achieving robustness to a wide range of morphological variability. We demonstrate SynthSeg on 5,000 scans of six modalities (including CT) and ten resolutions, where it exhibits unparallelled generalisation compared with supervised CNNs, state-of-the-art domain adaptation, and Bayesian segmentation. Finally, we demonstrate the generalisability of SynthSeg by applying it to cardiac MRI and CT scans.},
    html={https://www.sciencedirect.com/science/article/pii/S1361841523000506},
    pdf={https://arxiv.org/pdf/2107.09559.pdf},
    bibtex_show={true},
    abbr={Journal},
}

@inproceedings{billot_equivariant_2023,
    title = {Equivariant and Denoising CNNs to Decouple Intensity and Spatial Features for Motion Tracking in Fetal Brain MRI},
    author = {Billot, Benjamin and Moyer, Daniel and Karani, Neerav and Hoffmann, Malte and Abaci Turk, Esra and Grant, Ellen and Golland, Polina},
    booktitle = {MIDL short paper track},
    year = {2023},
    abstract = {Equivariance in convolutional neural networks (CNN) has been a long-sought property, as it would ensure robustness to expected effects in the data. Convolutional filters are by nature translation-equivariant, and rotation-equivariant kernels were proposed recently. While these filters can be paired with learnable weights to form equivariant networks (E-CNN), we show here that such E-CNNs have a limited learning capacity, which makes them fragile against even slight changes in intensity distribution. This sensitivity to intensity changes presents a major challenge in medical imaging where many noise sources can randomly corrupt the data, even for consecutive scans of the same subject. Here, we propose a hybrid architecture that successively decouples intensity and spatial features: we first remove irrelevant noise in the data with a denoising CNN, and then use an E-CNN to extract robust spatial features. We demonstrate our method for motion tracking in fetal brain MRI, where it considerably outperforms standard CNNs and E-CNNs.},
    html={https://openreview.net/forum?id=C7VKeiHeZT},
    pdf={https://openreview.net/pdf?id=C7VKeiHeZT},
    bibtex_show={true},
    abbr={Conf.},
}

@inproceedings{dey_anystar_2023,
    title = {{AnyStar}: {Domain} randomized universal star-convex {3D} instance segmentation},
    author = {Dey, Neel and Abulnaga, Mazdak, and Billot, Benjamin and Abaci Turk, Esra and Grant, Ellen and Dalca, Adrian and Golland, Polina},
    booktitle = {Winter Conference on Applications of Computer Vision},
    year = {2023},
    abstract = {Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or fine tuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C.elegans and P.dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in µCT, zebra fish brainn uclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, fine tuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.},
    html={https://arxiv.org/abs/2307.07044},
    pdf={https://arxiv.org/pdf/2307.07044.pdf},
    bibtex_show={true},
    abbr={Conf.},
}

@article{iglesias_quantitative_2023,
    author = {Iglesias, Juan Eugenio and Schleicher, Riana and Laguna, Sonia and Billot, Benjamin and Schaefer, Pamela and McKaig, Brenna and Goldstein, Joshua and Sheth, Kevin and Rosen, Matthew and Kimberly, Taylor},
    title = {Quantitative Brain Morphometry of Portable Low-Field-Strength MRI Using Super-Resolution Machine Learning},
    journal = {Radiology},
    volume = {306},
    number = {3},
    year = {2023},
    abstract = {Background Portable, low-field-strength (0.064-T) MRI has the potential to transform neuroimaging but is limited by low spatial resolution and low signal-to-noise ratio. Purpose To implement a machine learning super-resolution algorithm that synthesizes higher spatial resolution images (1-mm isotropic) from lower resolution T1-weighted and T2-weighted portable brain MRI scans, making them amenable to automated quantitative morphometry. Materials and Methods An external high-field-strength MRI data set (1-mm isotropic scans from the Open Access Series of Imaging Studies data set) and segmentations for 39 regions of interest (ROIs) in the brain were used to train a super-resolution convolutional neural network (CNN). Secondary analysis of an internal test set of 24 paired low- and high-field-strength clinical MRI scans in participants with neurologic symptoms was performed. These were part of a prospective observational study (August 2020 to December 2021) at Massachusetts General Hospital (exclusion criteria: inability to lay flat, body habitus preventing low-field-strength MRI, presence of MRI contraindications). Three well-established automated segmentation tools were applied to three sets of scans: high-field-strength (1.5–3 T, reference standard), low-field-strength (0.064 T), and synthetic high-field-strength images generated from the low-field-strength data with the CNN. Statistical significance of correlations was assessed with Student t tests. Correlation coefficients were compared with Steiger Z tests. Results Eleven participants (mean age, 50 years ± 14; seven men) had full cerebrum coverage in the images without motion artifacts or large stroke lesion with distortion from mass effect. Direct segmentation of low-field-strength MRI yielded nonsignificant correlations with volumetric measurements from high field strength for most ROIs (P > .05). Correlations largely improved when segmenting the synthetic images: P values were less than .05 for all ROIs (eg, for the hippocampus [r = 0.85; P < .001], thalamus [r = 0.84; P = .001], and whole cerebrum [r = 0.92; P < .001]). Deviations from the model (z score maps) visually correlated with pathologic abnormalities. Conclusion This work demonstrated proof-of-principle augmentation of portable MRI with a machine learning super-resolution algorithm, which yielded highly correlated brain morphometric measurements to real higher resolution images. © RSNA, 2022 Online supplemental material is available for this article. See also the editorial by Ertl-Wagner amd Wagner in this issue. An earlier incorrect version appeared online.},
    html = {https://pubs.rsna.org/doi/abs/10.1148/radiol.220522},
    pdf = {https://pubs.rsna.org/doi/epdf/10.1148/radiol.220522},
    bibtex_show={true},
    abbr={Journal},
}

@inproceedings{billot_robust_2022,
	title = {Robust {Segmentation} of {Brain} {MRI} in the {Wild} with {Hierarchical} {CNNs} and no {Retraining}},
	booktitle = {MICCAI},
	author = {Billot, Benjamin and Colin, Magdamo and Arnold, Steven E and Das, Sudeshna and Iglesias, Juan Eugenio},
	year = {2022},
	pages = {538--548},
	abstract = {Retrospective analysis of brain MRI scans acquired in the clinic has the potential to enable neuroimaging studies with sample sizes much larger than those found in research datasets. However, analysing such clinical images “in the wild” is challenging, since subjects are scanned with highly variable protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent advances in convolutional neural networks (CNNs) and domain randomisation for image segmentation, best represented by the publicly available method SynthSeg, may enable morphometry of clinical MRI at scale. In this work, we first evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000 scans acquired at Massachusetts General Hospital. We show that SynthSeg is generally robust, but frequently falters on scans with low signal-to-noise ratio or poor tissue contrast. Next, we propose SynthSeg, a novel method that greatly mitigates these problems using a hierarchy of conditional segmentation and denoising CNNs. We show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and state-of-the-art segmentation denoising methods. Finally, we apply our approach to a proof-of-concept volumetric study of ageing, where it closely replicates atrophy patterns observed in research studies conducted on high-quality, 1 mm, T1-weighted scans. The code and trained model are publicly available at https://github.com/BBillot/SynthSeg.},
    html={https://link.springer.com/chapter/10.1007/978-3-031-16443-9_52},
    pdf={https://arxiv.org/pdf/2203.01969.pdf},
    bibtex_show={true},
    abbr={Conf.},
}

@article{hoffmann_learning_2022,
    author={Hoffmann, Malte and Billot, Benjamin and Greve, Douglas N. and Iglesias, Juan Eugenio and Fischl, Bruce and Dalca, Adrian V.},
    journal={IEEE Transactions on Medical Imaging},
    title={SynthMorph: Learning Contrast-Invariant Registration Without Acquired Images},
    year={2022},
    volume={41},
    number={3},
    pages={543-558},
    abstract = {We introduce a strategy for learning image registration without acquired imaging data, producing powerful networks agnostic to contrast introduced by magnetic resonance imaging (MRI). While classical registration methods accurately estimate the spatial correspondence between images, they solve an optimization problem for every new image pair. Learning-based techniques are fast at test time but limited to registering images with contrasts and geometric content similar to those seen during training. We propose to remove this dependency on training data by leveraging a generative strategy for diverse synthetic label maps and images that exposes networks to a wide range of variability, forcing them to learn more invariant features. This approach results in powerful networks that accurately generalize to a broad array of MRI contrasts. We present extensive experiments with a focus on 3D neuroimaging, showing that this strategy enables robust and accurate registration of arbitrary MRI contrasts even if the target contrast is not seen by the networks during training. We demonstrate registration accuracy surpassing the state of the art both within and across contrasts, using a single model. Critically, training on arbitrary shapes synthesized from noise distributions results in competitive performance, removing the dependency on acquired data of any kind. Additionally, since anatomical label maps are often available for the anatomy of interest, we show that synthesizing images from these dramatically boosts performance, while still avoiding the need for real intensity images. Our code is available at doic https://w3id.org/synthmorph.},
    html={https://ieeexplore.ieee.org/abstract/document/9552865},
    pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9552865},
    bibtex_show={true},
    abbr={Journal},
}

@article{shapiro_in_vivo_2022,
    author={Shapiro, Noah and Todd, Emily and Billot, Benjamin and Cash, David and Iglesias, Juan Eugenio and Jason, Warren and Rohrer, Jonathan and Bocchetta, Martina},
    journal={NeuroImage},
    title={In vivo hypothalamic regional volumetry across the frontotemporal dementia spectrum},
    year={2022},
    volume={35},
    abstract = {Frontotemporal dementia (FTD) is a spectrum of diseases characterised by language, behavioural and motor symptoms. Among the different subcortical regions implicated in the FTD symptomatology, the hypothalamus regulates various bodily functions, including eating behaviours which are commonly present across the FTD spectrum. The pattern of specific hypothalamic involvement across the clinical, pathological, and genetic forms of FTD has yet to be fully investigated, and its possible associations with abnormal eating behaviours have yet to be fully explored.},
    html={https://www.sciencedirect.com/science/article/pii/S2213158222001498},
    pdf={https://pdf.sciencedirectassets.com/282794/1-s2.0-S2213158222X0003X/1-s2.0-S2213158222001498/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEL3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICH2CHRhSqklhFUw3o9JD%2B6M%2BtE0nh1J2xmRDGgjKZB7AiEA9uEqw7yQeZ9Xqeh75e6Um6tsMvGgkc%2BzXxpDz9lz5%2FUqvAUI5f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDAV1VQ4d%2FrFqId2nfCqQBSFGfAPpwySoa6mql2H8TbrVK1vrc7CvzUF21V3imOOMUlB56GIgTGdJY%2Fwz%2B6afa%2FO7WxzkBsoUHqNDrF58Bvo07YwRrkPGfqlNWKxmjxMD9yGF47SoPx6YR2LI18WxylLaAgrd7hl%2FVPJsl%2FyOYEOeSmZXf7Ye4sjWjXHmShFcC9uiEyDfVqkB9J1BhG9LCFYHs0RofcUDF0nMTDjqfOGjSDcSMzJqNTMobDidCVUpy7Xgx0s8bN1xm8sz8E8sFC7gEqmr%2BnJpSrPTBk80O1X3j7G95HOk%2BMolbwXlZI7WYgiyNYM11UdYw%2FcFQgFmBYCZLnVAHlaUrELDUgbR0Z98LEeTBdZ7e5RFCoSlkDZD7Jr1iA%2ByLelgpekXncROPM3hbdh7rspmwwZB8I7sB4Hg92v6MIn2aYajoyhUTvl6sFDN%2F0oU6S32mHELyUPoqV062j%2F58QEJ8%2BS0ttsdVfwHC2sLmD450AkJ0rBRted%2B17Xnmi8QllnZ9iX2Mmo6awXKmYCPnAFNSpv19Shi2OfrPoZee5NV21Sp%2FYVRJPHw3s0Ij5GSymeNDJIG1cau9wJ5iAus3QN2dDSbHjXQ7WHKuXQIPG1UNdFzDnGzR9tCuHrbHEBNZy5IqOfEl5f994NVLt0yqJtwOBpcMoTWy54911NCsTlHiJSEs7UrG6l4vmptrJg0m6XelLytPPIyJbElUq2Z24xVocghdKRzPe3XGkJh87Wxtej96FjonOE8q6ZX3kxzYvLeitNBXJMq%2FUQpRvpgEA65R9juSWigo1HLVqXR2Z5g6H7mCrn%2Btj7snY6dJJzadfYkxXxbiep1UZphqDJdIg4w3zRosXJNS58ecB%2BP9GtTZKCYK6mv0WUHMI78gaoGOrEBBgUBd%2BY14zvC8aXnLcq4%2B9XgDouQKwzFXivZr7YmUMwr6o5p2rTZdLEOG0%2FlXBVCMNsl75APt3yK42wUjxXTmfcpmd%2FMh9iNp2xokxXlZmT%2FWG6p5aKVlxOHpIB5hYbhUtCC7pBkNcbCnwXHZF%2FPgVwJIyBqF20vBloPkO%2F63OkidwVimztDuIKISyXR%2B0kmojQuFN2lmZdcZy4gbbVI7ZKtUsnksslgNlb1mGajt12L&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231031T043353Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYVYLOOJ5A%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=96b00868bd217029239d7b84393890ef96b7ba9472ac6b75fd552363f2b7c4d6&hash=4cc9e0092b213c9bc65e08c467d8fd428d1d4dbb24f3a5e91822b2e3c6eea1a0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2213158222001498&tid=spdf-22a2ffd4-5201-42d0-82f4-439a07d6aee6&sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f165c520a5950020c&rr=81e92eb35e3b305d&cc=us},
    bibtex_show={true},
    abbr={Journal},
}

@inproceedings{laguna_super_2022,
    title = {Super-resolution of portable low-field MRI in real scenarios: integration with denoising and domain adaptation},
    author = {Laguna, Sonia and Schleicher, Riana and Billot, Benjamin and Schaefer, Pamela and McKaig, Brenna and Goldstein, Joshua and Sheth, Kevin and Rosen, Matthew and Kimberly, Taylor and Iglesias, Juan Eugenio},
    booktitle = {MIDL},
    year = {2022},
    abstract = {Portable low-field MRI has the potential to revolutionize neuroimaging, by enabling point-of-care imaging and affordable scanning in underserved areas. The lower resolution and signal-to-noise ratio of these scans preclude image analysis with existing tools. Super-resolution (SR) methods can overcome this limitation, but: (i) training with downsampled high-field scans fails to generalize; and (ii) training with paired low/high-field data is hard due to the lack of perfectly aligned images. Here, we present an architecture that combines denoising, SR and domain adaptation modules to tackle this problem. The denoising and SR components are pretrained in a supervised fashion with large amounts of existing high-resolution data, whereas unsupervised learning is used for domain adaptation and end-to-end finetuning. We present preliminary results on a dataset of 11 low-field scans. The results show that our method enables segmentation with existing tools, which yield ROI volumes that correlate strongly with those derived from high-field scans (ρ > 0.8).},
    html = {https://openreview.net/forum?id=pinw0Gcot4T},
    pdf={https://openreview.net/pdf?id=pinw0Gcot4T},
    bibtex_show={true},
    abbr={Conf.},
}

@inproceedings{billot_joint_2021,
	title = {Joint {Segmentation} {Of} {Multiple} {Sclerosis} {Lesions} {And} {Brain} {Anatomy} {In} {MRI} {Scans} {Of} {Any} {Contrast} {And} {Resolution} {With} {CNNs}},
	booktitle = {ISBI},
	author = {Billot, Benjamin and Cerri, Stefano and Van Leemput, Koen and Dalca, Adrian and Iglesias, Juan Eugenio},
    year = {2021},
    pages = {1971--1974},
    abstract = {We present the first deep learning method to segment Multiple Sclerosis lesions and brain structures from MRI scans of any (possibly multimodal) contrast and resolution. Our method only requires segmentations to be trained (no images), as it leverages the generative model of Bayesian segmentation to generate synthetic scans with simulated lesions, which are then used to train a CNN. Our method can be retrained to segment at any resolution by adjusting the amount of synthesised partial volume. By construction, the synthetic scans are perfectly aligned with their labels, which enables training with noisy labels obtained with automatic methods. The training data are generated on the fly, and aggressive augmentation (including artefacts) is applied for improved generalisation. We demonstrate our method on two public datasets, comparing it with a state-of-the-art Bayesian approach implemented in FreeSurfer, and dataset specific CNNs trained on real data. The code is available at https://github.com/BBillot/SynthSeg.},
    html={https://ieeexplore.ieee.org/abstract/document/9434127},
    pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9434127},
    bibtex_show={true},
    abbr={Conf.},
}

@article{iglesias_joint_2021,
    title = {Joint super-resolution and synthesis of 1mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast},
    journal = {NeuroImage},
    volume = {237},
    year = {2021},
    author = {Iglesias, Juan Eugenio and Billot, Benjamin and Balbastre, Yaël and Tabari, Azadeh  and Conklin, John and {Gilberto González}, R. and Alexander, Daniel C. and Golland, Polina  and Edlow, Brian L. and Fischl, Bruce },
    abstract = {Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well-typically requiring T1-weighted images (e.g., MP-RAGE scans). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing in clinical settings every year. In turn, the inability to quantitatively analyze these scans hinders the adoption of quantitative neuro imaging in healthcare, and also precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in convolutional neural networks (CNNs) are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the specific combination of contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols – even within sites. In this article, we present SynthSR, a method to train a CNN that receives one or more scans with spaced slices, acquired with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, beyond rigid coregistration of the input scans. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution real images of the input contrasts. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at https://github.com/BBillot/SynthSR.},
    html={https://www.sciencedirect.com/science/article/pii/S1053811921004833},
    pdf={https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811921X00109/1-s2.0-S1053811921004833/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCurdpBWKmb9Vvz3Con4IxCSFg1dIqcK3PnXwNTZczxggIhAI7MjW0YiadYnFYG%2B26ycu2oorEQ%2BY3%2FoiaW3f3DlzIbKroFCOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igwc4KoXF0GCEkoCU7IqjgV5IWiU61oKEdQrVBJOwRhrtfj6okHU%2FmKWeg1lS3XMWUAjDv2kXYm75bWWlTsChK%2F7CvqUirj4qcoxCNRwU91lkBjT4BTNZ%2BfuMpu2zmfZs9yW1hkzVCSJ0co%2Fafg4LY9wP2%2FKsJ51Ns0R2R7sSnNkgj0BNJHwaby4BHCe9iQgiU%2BHWdYXpKnEut7VpxnJL%2FSvgXHUFaofKiYal0sdisK%2BS7FOwP9xU9O8EXTq4R0wwZpwrD3XQebsfOU9ZMSh5Zp18hldjV1wylQnYV75tjE49wZ0Hzmep7lqwqJe%2B1bGmXHbBzcVEp4NtEgz83flezaQ5jUQImAC1tpsmLa%2Bdyp6vJrEUQ9Xqkr1ZM8umIvI%2FXcKL23eeD1oZ4lGbgTLGjAMwWeyRpu9%2Fep6Brvd7I%2BeRspSkzCwCPGA%2FN%2Fqs9NgW1jzwFi7V4A5sBW0eGYBhZhRH4P1%2BIAtRrK6ReRyqE%2Fgays0XWWDRKC8evx7%2FxK4pktTcELTAHMC%2BDvQ2EfHgMH8TuZ2WWsOYBOPTbVa6sYDyLFk5V1Knx%2B2avR8j3D7Mvr7FzwQ1W68lx1VmhPbn%2FfmSr50VxqcGh2N87WwiAEm%2FDQ98r8bUEdEsTmV383QVV7PpwIi0SAZzTFbLzxBBq8XxR9KcGLczwOB%2FNiRMRkGn2vjrnSZ9CZCoG1%2BDe3TXIH43wkVSTXHOOuPbXxXshBBoqYPcv7u5Z0WJt4el0soO2I5A8urHH6fqgdOnGCmorS9G7%2BG7EfRCuhMWk2%2F6H2ARrTZqGAVhF5N1LAp83FV%2FoDlWJAPglbO1KFSEv%2Foa3DxZUKq%2BYki1WiBMuxAkWpmc%2BDMrtkIqxpMrtMR%2BOox8IuTdLmNjAo2DNFBPEYwp8eBqgY6sAH3%2Be%2FNd7sairW9Rb6hTQiCPuMzL%2BsHe26bp09p3NPyd22rFfo4syiBgmJA62wg14wuIwetphkj79HWqLxB3mX76OCdCcizDjmzcGh6cDXzk2o0TeAbZfXrlJtyDMbtvxhUvW5ShtIRGtUe17CwPdRFZdqJO3dhd6uqkLf3jqu0vYz3YZ6LrjqrIuZrt5yS9iKuHDrb6NdilL6ixI5CAVkveEkF0BJt%2F%2Fmj2tarHl1JPA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231031T030041Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4K7ONGJH%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=3d536b3e2c078954dc51a4c8f30e8ce1c6bf88aacbde4725ec1741d360ac9361&hash=a0b559d36419a6dbcc8a8288a407afbd641e13862975e12eb3b6e810dbf639fd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1053811921004833&tid=spdf-45bdd306-d811-4ec3-9007-d63ad0f71b0f&sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f165c520a5803515c&rr=81e8a62c0b143ba5&cc=us},
    bibtex_show={true},
    abbr={Journal},
}

@article{greve_deep_2021,
    title = {A deep learning toolbox for automatic segmentation of subcortical limbic structures from MRI images},
    author = {Greve, Douglas N. and Billot, Benjamin and Cordero, Devani and Hoopes, Andrew and Hoffmann, Malte and Dalca, Adrian and Fischl, Bruce and Iglesias, Juan Eugenio and Augustinack, Jean},
    journal = {NeuroImage},
    volume = {244},
    year = {2021},
    abstract = {A tool was developed to automatically segment several subcortical limbic structures (nucleus accumbens, basal forebrain, septal nuclei, hypothalamus without mammillary bodies, the mammillary bodies, and fornix) using only a T1-weighted MRI as input. This tool fills an unmet need as there are few, if any, publicly available tools to segment these clinically relevant structures. A U-Net with spatial, intensity, contrast, and noise augmentation was trained using 39 manually labeled MRI data sets. In general, the Dice scores, true positive rates, false discovery rates, and manual-automatic volume correlation were very good relative to comparable tools for other structures. A diverse data set of 698 subjects were segmented using the tool; evaluation of the resulting labelings showed that the tool failed in less than 1% of cases. Test-retest reliability of the tool was excellent. The automatically segmented volume of all structures except mammillary bodies showed effectiveness at detecting either clinical AD effects, age effects, or both. This tool will be publicly released with FreeSurfer (surfer.nmr.mgh.harvard.edu/fswiki/ScLimbic). Together with the other cortical and subcortical limbic segmentations, this tool will allow FreeSurfer to provide a comprehensive view of the limbic system in an automated way.},
    html = {https://www.sciencedirect.com/science/article/pii/S1053811921008831},
    pdf={https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811921X00171/1-s2.0-S1053811921008831/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDUU6FgpVGGKLMbPkSucg%2B3HL3RtE%2FpiRMODJZdD4KmHAIhAJ%2F4Iy57IJRJ0IpDJabD2%2FSVa2GbsyYa0KgnhgkUDfJuKrsFCOT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igw7V0oDxdSsw%2F%2FFMoQqjwW51KJc7tCrVZ74Jqal1%2BjNEj0EDvvIuP1iwks6wavy7%2Fc5J1bF7ZJaG5AibBfIz0ITVDpe4IKEyqt%2Fh63t8ATWNOVLJ6zA%2BkukRfP8wbcLSorVmVl%2Fo9kK7kiliJfUKMczY5YbpuEZo0Vha3cfgIHzBzOXTTWICZboks1OLXlpsb5tGJyzEzJ1xHla70SVV%2BieLOrMoFcBt%2FsEffWfwycEBrhxtfks3D1BiA9QH%2B7oyLkVnzuDkNm3kh0QkdbNQoKBLWYMfwawuw%2BLEbrSAbBpnNgKOG13YLqbEq4lIhXHX2ZgtjD6jnQDMuHozL2WULrz9l6yTuT%2Fx6etoK%2FKqXQKyxqurt%2FbT7Br221RP3riZGv75xu%2Fkn8i9G4CJOdeTkmFVYGAj7YbvZeS5QucPaY6EnXc%2FKJih6KXnQOMZc6YQoqZleNBvpEswBMrf%2FgI3FrdbFQU3%2FoQFa3axTx%2F6u2i%2FbGgpxJ4nP0wsE4NVj9MEZyJ8vrBkf1inTbfGZVP10inlwD8IA8IhG1wpVhHzGai4tYwn%2BZ6uwcsIjy%2FKWDSUM%2FjGVB2jw3WiT%2FDYfcaBbDK7qTg6GhBDMGaGjVGwiaeYDSX9p0XD%2FlaRJnk9YISF4MVdP9pAixLLTBocbpiKC4kVXnwzG1%2Bb8cvhPsB4id8QQEoaFhw7%2BrPVk6hC34dEJQ9gwEaZFJAmsIJx%2FxzxZ2K5S1hB7Bpplvtz2V20zIWEt93a5AF8V5CB8%2BCpiPCbBiZRGI66F%2FwKFWYWsC0Na3QtgR3PMd09SMDFrikDEDxCUQz60wfAo0ZgLha0RyCwsyOBTVj8qzmAvzTzRIxQeGHdRnL3sgWs%2BigojxjL%2FTnZhagvdW43W7QvsC3DOM4MNDkgaoGOrAB6rLeuLN3wkxLlPUiBY6C6njs8H1NSTYRPkVP9qgte%2FdyjIOJnoeUNW2IOgX5Q8F5qhLhB4P5VQLvQUu7%2FbwOJzZjNJiApBE1m1Og2qg7oiUtOoEdKaA1kM0x2%2BzpIny3iXAslZcEw6Fxync%2BmDz%2FRt%2BH6DKyuK4X1l4ngFyISffUWXH922HlGDyFhdKMM9HJB0WjyIfBkSkYJvykdGamorBwLtGL5%2BecaVVyeduzWbo%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231031T042233Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYYMII4FWV%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=eae8655680b99d7528978ce2691d7fc8add4c3c05ad9a2ad15090734e14b45c6&hash=e99b26223746ccf492456584a05e4ec9a658e9e5f5293ed60fb9ab54484e6be5&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1053811921008831&tid=spdf-7cf7382a-35f3-4d63-a3c5-c5c8a090081d&sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f165c520a5953025f&rr=81e91e1bfc334cd0&cc=us},
    bibtex_show={true},
    abbr={Journal},
}

@inproceedings{billot_partial_2020,
    title = {Partial {Volume} {Segmentation} of {Brain} {MRI} {Scans} of {Any} {Resolution} and {Contrast}},
    booktitle = {MICCAI},
    author = {Billot, Benjamin and Robinson, Eleanor and Dalca, Adrian V. and Iglesias, Juan Eugenio},
    year = {2020},
    pages = {177--187},
    abstract = {Partial voluming (PV) is arguably the last crucial unsolved problem in Bayesian segmentation of brain MRI with probabilistic atlases. PV occurs when voxels contain multiple tissue classes, giving rise to image intensities that may not be representative of any one of the underlying classes. PV is particularly problematic for segmentation when there is a large resolution gap between the atlas and the test scan, e.g., when segmenting clinical scans with thick slices, or when using a high-resolution atlas. Forward models of PV are realistic and simple, as they amount to blurring and subsampling a high resolution (HR) volume into a lower resolution (LR) scan. Unfortunately, segmentation as Bayesian inference quickly becomes intractable when “inverting” this forward PV model, as it requires marginalizing over all possible anatomical configurations of the HR volume. In this work, we present PV-SynthSeg, a convolutional neural network (CNN) that tackles this problem by directly learning a mapping between (possibly multi-modal) LR scans and underlying HR segmentations. PV-SynthSeg simulates LR images from HR label maps with a generative model of PV, and can be trained to segment scans of any desired target contrast and resolution, even for previously unseen modalities where neither images nor segmentations are available at training. PV-SynthSeg does not require any preprocessing, and runs in seconds. We demonstrate the accuracy and flexibility of our method with extensive experiments on three datasets and 2,680 scans. The code is available at https://github.com/BBillot/SynthSeg.},
    html={https://link.springer.com/chapter/10.1007/978-3-030-59728-3_18},
    pdf={https://arxiv.org/pdf/2004.10221.pdf},
    bibtex_show={true},
    abbr={Conf.},
}

@inproceedings{billot_learning_2020,
    title = {A {Learning} {Strategy} for {Contrast}-agnostic {MRI} {Segmentation}},
    booktitle = {MIDL},
    author = {Billot, Benjamin and Greve, Douglas N. and Leemput, Koen Van and Fischl, Bruce and Iglesias, Juan Eugenio and Dalca, Adrian},
    year = {2020},
    pages = {75--93},
    abstract = {We present a deep learning strategy for contrast-agnostic semantic segmentation of unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources. In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic scans of widely varying contrasts on the fly during training. These scans are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any specific MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four MR contrasts. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at: https://github.com/BBillot/SynthSeg.},
    html={https://proceedings.mlr.press/v121/billot20a.html},
    pdf={https://proceedings.mlr.press/v121/billot20a/billot20a.pdf},
    bibtex_show={true},
    abbr={Conf.},
}

@article{billot_automated_2020,
    title = {Automated segmentation of the hypothalamus and associated subunits in brain {MRI}},
    volume = {223},
    journal = {NeuroImage},
    author = {Billot, Benjamin and Bocchetta, Martina and Todd, Emily and Dalca, Adrian V. and Rohrer, Jonathan D. and Iglesias, Juan Eugenio},
    year = {2020},
    abstract = {Despite the crucial role of the hypothalamus in the regulation of the human body, neuroimaging studies of this structure and its nuclei are scarce. Such scarcity partially stems from the lack of automated segmentation tools, since manual delineation suffers from scalability and reproducibility issues. Due to the small size of the hypothalamus and the lack of image contrast in its vicinity, automated segmentation is difficult and has been long neglected by widespread neuroimaging packages like FreeSurfer or FSL. Nonetheless, recent advances in deep machine learning are enabling us to tackle difficult segmentation problems with high accuracy. In this paper we present a fully automated tool based on a deep convolutional neural network, for the segmentation of the whole hypothalamus and its subregions from T1-weighted MRI scans. We use aggressive data augmentation in order to make the model robust to T1-weighted MR scans from a wide array of different sources, without any need for preprocessing. We rigorously assess the performance of the presented tool through extensive analyses, including: inter- and intra-rater variability experiments between human observers; comparison of our tool with manual segmentation; comparison with an automated method based on multi-atlas segmentation; assessment of robustness by quality control analysis of a larger, heterogeneous dataset (ADNI); and indirect evaluation with a volumetric study performed on ADNI. The presented model outperforms multi-atlas segmentation scores as well as inter-rater accuracy level, and approaches intra-rater precision. Our method does not require any preprocessing and runs in less than a second on a GPU, and approximately 10 seconds on a CPU. The source code as well as the trained model are publicly available at https://github.com/BBillot/hypothalamus_seg, and will also be distributed with FreeSurfer.},
    html={https://www.sciencedirect.com/science/article/pii/S1053811920307734},
    pdf={https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811920X00152/1-s2.0-S1053811920307734/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDbQ1bOpbB5Hi0qLKePBehucNkzSfcivIN84gu33JeJ7AiAApU8H8aQzDMazpvTStc9o2u4lmet9XbNYXlBHx%2B4QZSq7BQjj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMtnJ0y1JSIftJ5V%2FOKo8FnRJNTqDOkFymNEUBTqIY%2FmN%2F5z79UWrIvJKOVbV6XbJmMc%2FLO3RrfFWS2cvo2DVKWYm1DecxkEFmD05VEcbyDESSkjh5Pfuob3uw0%2Fyu7c32553q378rZTcA%2BKZWvZOPMderOuJggBLKkgzjVacV5T5CdxLNOhK%2FL3BG7RcuMhtUWpkK7g6Cis4bywGQuKX%2Bk9Mt8Rb%2Bps6ekAkI%2BKKVn79AGw1gqP6CA3%2BZTqUZXBqx76eUIEFmOvcRNwznRpiUdIbaaPNIaDNoZj5W6i2HipSF0JJ8yhAWNo8%2Fvmh6Z1rL9LhGdqp6EsvsB38qZXf0J7DFpydfNM%2FKZfhzgtMgleukwaYmzm8MkcVMzTNgfmrBgFqgY4u0nQVxL9TbBIXRlLd0fEcDGMh03Ho5p2GRtCWClnp%2B%2F%2FdcmJ7VhrCbVDJ2LFpWT%2F3ttBYj5gnXYUd7B1Q5dwlweB36hiIj9gGWx%2F4FQl9PAOwMMjCLQcx5ec2T1Vr%2B03wTnDF9rBoiJXdc3X0yG2qc8QLapZLlziPw5Rk4Pw7RCNUKPyH3iVvucuVMFVBWYKavCmCjf69lcoB0wNsKLj3FrzkEkICM97BhJzMYOodOYDyHp9wszr8kIhLer3OTwIFko9VN3YnmIPcgKSyyxf12zZdtfc4NCm%2FzUU8gXSPQdhjLrijFrr1wxJ9WL1dhfi3T1rbhmtIIulhaZ14aTF5eo8pFRMzTmwoJOIcGVz1IeY37QC%2FHVVH3BEKXOV5dr7lenHDNucXoqXJf1I%2FLTvyHXjk0AS4fw0x9jnRL%2Blafk10NABxu5RhFP3EchTDoElPwli8VEM12hUPCVLBtVmlZHwCj6iwSpS%2BpyMMb8IhgGYLJICKhJuv%2FOjDltoGqBjqyAQcJZkcyYwzEYy16YkUqD4wenaLMB02OClXvN5DH6eWgB7OgRipu%2BE%2B2bP3d862SaRS1WSzByywTn%2Fln5b0lPGF4Bn%2BaeJtIqA8ffLdsN9unoeoAK6VOwGpeepsOSAmY3RQDntZXs7nQzIXVM3fU5yo%2BJykOv%2BfkMxRs0anZxGiJEYO5RR8NiWdr8L8cTtuyWXQ5Z5cT7%2F5dw0v%2FqUUoXhbSYLA50NvTaCr3hGQTe9S8mBQ%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231031T024317Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXDXUNUXW%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=a32797cbde3a7632c9068523aadc6c71f5d5185a83abe68308666bc660be41fe&hash=0445c59e26c3da9daba534780b03411e9f83b475ca74eeca87cdd405248cb157&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1053811920307734&tid=spdf-bd20a4d3-504c-422a-9265-3bf6b5d22771&sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f165c520a585a040f&rr=81e88caf886f4cf9&cc=us},
    bibtex_show={true},
    abbr={Journal},
}