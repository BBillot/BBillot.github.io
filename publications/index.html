<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Benjamin Billot</title> <meta name="author" content="Benjamin Billot"> <meta name="description" content="Personal website of Benjamin Billot "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bbillot.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Benjamin </span><span class="font-weight-bold">Billot </span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="billot_robust_2023" class="col-sm-8"> <div class="title">Robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain MRI datasets</div> <div class="author"> <em>Benjamin Billot</em>, Magdamo Colin, You Cheng, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sudeshna Das, Juan Eugenio Iglesias' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences (PNAS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2216399120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.pnas.org/doi/epdf/10.1073/pnas.2216399120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Every year, millions of brain MRI scans are acquired in hospitals, which is a figure considerably larger than the size of any research dataset. Therefore, the ability to analyze such scans could transform neuroimaging research. Yet, their potential remains untapped since no automated algorithm is robust enough to cope with the high variability in clinical acquisitions (MR contrasts, resolutions, orientations, artifacts, and subject populations). Here, we present SynthSeg+, an AI segmentation suite that enables robust analysis of heterogeneous clinical datasets. In addition to whole-brain segmentation, SynthSeg+ also performs cortical parcellation, intracranial volume estimation, and automated detection of faulty segmentations (mainly caused by scans of very low quality). We demonstrate SynthSeg+ in seven experiments, including an aging study on 14,000 scans, where it accurately replicates atrophy patterns observed on data of much higher quality. SynthSeg+ is publicly released as a ready-to-use tool to unlock the potential of quantitative morphometry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">billot_robust_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Robust} machine learning segmentation for large-scale analysis of heterogeneous clinical brain {MRI} datasets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Colin, Magdamo and Cheng, You and Das, Sudeshna and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Proceedings} of the {National} {Academy} of {Sciences} ({PNAS})}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{120}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="iglesias_synthsr_2023" class="col-sm-8"> <div class="title">SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry</div> <div class="author"> Juan E. Iglesias, <em>Benjamin Billot</em>, Yaël Balbastre, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Colin Magdamo, Steven E. Arnold, Sudeshna Das, Brian L. Edlow, Daniel C. Alexander, Polina Golland, Bruce Fischl' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Science Advances</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.science.org/doi/abs/10.1126/sciadv.add3607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.science.org/doi/epdf/10.1126/sciadv.add3607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Every year, millions of brain magnetic resonance imaging (MRI) scans are acquired in hospitals across the world. These have the potential to revolutionize our understanding of many neurological diseases, but their morphometric analysis has not yet been possible due to their anisotropic resolution. We present an artificial intelligence technique, “SynthSR,” that takes clinical brain MRI scans with any MR contrast (T1, T2, etc.), orientation (axial/coronal/sagittal), and resolution and turns them into high-resolution T1 scans that are usable by virtually all existing human neuroimaging tools. We present results on segmentation, registration, and atlasing of &gt;10,000 scans of controls and patients with brain tumors, strokes, and Alzheimer’s disease. SynthSR yields morphometric results that are very highly correlated with what one would have obtained with high-resolution T1 scans. SynthSR allows sample sizes that have the potential to overcome the power limitations of prospective research studies and shed new light on the healthy and diseased human brain. A public AI tool turns clinical brain MRI of any resolution/contrast into 1mm T1 scans compatible with neuroimaging pipelines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">iglesias_synthsr_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iglesias, Juan E. and Billot, Benjamin and Balbastre, Yaël and Magdamo, Colin and Arnold, Steven E. and Das, Sudeshna and Edlow, Brian L. and Alexander, Daniel C. and Golland, Polina and Fischl, Bruce}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Science Advances}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{eadd3607}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="billot_synthseg_2023" class="col-sm-8"> <div class="title">SynthSeg: Segmentation of brain MRI scans of any contrast and resolution without retraining</div> <div class="author"> <em>Benjamin Billot</em>, Douglas N. Greve, Oula Puonti, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Axel Thielscher, Koen Van Leemput, Bruce Fischl, Adrian V. Dalca, Juan Eugenio Iglesias' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Medical Image Analysis</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841523000506" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2107.09559.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Despite advances in data augmentation and transfer learning, convolutional neural networks (CNNs) difficultly generalise to unseen domains. When segmenting brain scans, CNNs are highly sensitive to changes in resolution and contrast: even within the same MRI modality, performance can decrease across datasets. Here we introduce SynthSeg, the first segmentation CNN robust against changes in contrast and resolution. SynthSeg is trained with synthetic data sampled from a generative model conditioned on segmentations. Crucially, we adopt a domain randomisation strategy where we fully randomise the contrast and resolution of the synthetic training data. Consequently, SynthSeg can segment real scans from a wide range of target domains without retraining or fine-tuning, which enables straightforward analysis of huge amounts of heterogeneous clinical data. Because SynthSeg only requires segmentations to be trained (no images), it can learn from labels obtained by automated methods on diverse populations (e.g., ageing and diseased), thus achieving robustness to a wide range of morphological variability. We demonstrate SynthSeg on 5,000 scans of six modalities (including CT) and ten resolutions, where it exhibits unparallelled generalisation compared with supervised CNNs, state-of-the-art domain adaptation, and Bayesian segmentation. Finally, we demonstrate the generalisability of SynthSeg by applying it to cardiac MRI and CT scans.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">billot_synthseg_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SynthSeg: {Segmentation} of brain {MRI} scans of any contrast and resolution without retraining}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Greve, Douglas N. and Puonti, Oula and Thielscher, Axel and Van Leemput, Koen and Fischl, Bruce and Dalca, Adrian V. and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Medical} {Image} {Analysis}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{86}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="billot_equivariant_2023" class="col-sm-8"> <div class="title">Equivariant and Denoising CNNs to Decouple Intensity and Spatial Features for Motion Tracking in Fetal Brain MRI</div> <div class="author"> <em>Benjamin Billot</em>, Daniel Moyer, Neerav Karani, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Malte Hoffmann, Esra Abaci Turk, Ellen Grant, Polina Golland' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In MIDL short paper track</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=C7VKeiHeZT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=C7VKeiHeZT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Equivariance in convolutional neural networks (CNN) has been a long-sought property, as it would ensure robustness to expected effects in the data. Convolutional filters are by nature translation-equivariant, and rotation-equivariant kernels were proposed recently. While these filters can be paired with learnable weights to form equivariant networks (E-CNN), we show here that such E-CNNs have a limited learning capacity, which makes them fragile against even slight changes in intensity distribution. This sensitivity to intensity changes presents a major challenge in medical imaging where many noise sources can randomly corrupt the data, even for consecutive scans of the same subject. Here, we propose a hybrid architecture that successively decouples intensity and spatial features: we first remove irrelevant noise in the data with a denoising CNN, and then use an E-CNN to extract robust spatial features. We demonstrate our method for motion tracking in fetal brain MRI, where it considerably outperforms standard CNNs and E-CNNs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">billot_equivariant_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Equivariant and Denoising CNNs to Decouple Intensity and Spatial Features for Motion Tracking in Fetal Brain MRI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Moyer, Daniel and Karani, Neerav and Hoffmann, Malte and Abaci Turk, Esra and Grant, Ellen and Golland, Polina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MIDL short paper track}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="dey_anystar_2023" class="col-sm-8"> <div class="title">AnyStar: Domain randomized universal star-convex 3D instance segmentation</div> <div class="author"> Neel Dey, Mazdak Abulnaga, <em>Benjamin Billot</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Esra Abaci Turk, Ellen Grant, Adrian Dalca, Polina Golland' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Winter Conference on Applications of Computer Vision</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2307.07044" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2307.07044.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or fine tuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C.elegans and P.dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in µCT, zebra fish brainn uclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, fine tuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dey_anystar_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{AnyStar}: {Domain} randomized universal star-convex {3D} instance segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dey, Neel and Abulnaga, Mazdak and Billot, Benjamin and Abaci Turk, Esra and Grant, Ellen and Dalca, Adrian and Golland, Polina}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Winter Conference on Applications of Computer Vision}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="iglesias_quantitative_2023" class="col-sm-8"> <div class="title">Quantitative Brain Morphometry of Portable Low-Field-Strength MRI Using Super-Resolution Machine Learning</div> <div class="author"> Juan Eugenio Iglesias, Riana Schleicher, Sonia Laguna, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Benjamin Billot, Pamela Schaefer, Brenna McKaig, Joshua Goldstein, Kevin Sheth, Matthew Rosen, Taylor Kimberly' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Radiology</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubs.rsna.org/doi/abs/10.1148/radiol.220522" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://pubs.rsna.org/doi/epdf/10.1148/radiol.220522" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Background Portable, low-field-strength (0.064-T) MRI has the potential to transform neuroimaging but is limited by low spatial resolution and low signal-to-noise ratio. Purpose To implement a machine learning super-resolution algorithm that synthesizes higher spatial resolution images (1-mm isotropic) from lower resolution T1-weighted and T2-weighted portable brain MRI scans, making them amenable to automated quantitative morphometry. Materials and Methods An external high-field-strength MRI data set (1-mm isotropic scans from the Open Access Series of Imaging Studies data set) and segmentations for 39 regions of interest (ROIs) in the brain were used to train a super-resolution convolutional neural network (CNN). Secondary analysis of an internal test set of 24 paired low- and high-field-strength clinical MRI scans in participants with neurologic symptoms was performed. These were part of a prospective observational study (August 2020 to December 2021) at Massachusetts General Hospital (exclusion criteria: inability to lay flat, body habitus preventing low-field-strength MRI, presence of MRI contraindications). Three well-established automated segmentation tools were applied to three sets of scans: high-field-strength (1.5–3 T, reference standard), low-field-strength (0.064 T), and synthetic high-field-strength images generated from the low-field-strength data with the CNN. Statistical significance of correlations was assessed with Student t tests. Correlation coefficients were compared with Steiger Z tests. Results Eleven participants (mean age, 50 years ± 14; seven men) had full cerebrum coverage in the images without motion artifacts or large stroke lesion with distortion from mass effect. Direct segmentation of low-field-strength MRI yielded nonsignificant correlations with volumetric measurements from high field strength for most ROIs (P &gt; .05). Correlations largely improved when segmenting the synthetic images: P values were less than .05 for all ROIs (eg, for the hippocampus [r = 0.85; P &lt; .001], thalamus [r = 0.84; P = .001], and whole cerebrum [r = 0.92; P &lt; .001]). Deviations from the model (z score maps) visually correlated with pathologic abnormalities. Conclusion This work demonstrated proof-of-principle augmentation of portable MRI with a machine learning super-resolution algorithm, which yielded highly correlated brain morphometric measurements to real higher resolution images. © RSNA, 2022 Online supplemental material is available for this article. See also the editorial by Ertl-Wagner amd Wagner in this issue. An earlier incorrect version appeared online.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">iglesias_quantitative_2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iglesias, Juan Eugenio and Schleicher, Riana and Laguna, Sonia and Billot, Benjamin and Schaefer, Pamela and McKaig, Brenna and Goldstein, Joshua and Sheth, Kevin and Rosen, Matthew and Kimberly, Taylor}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantitative Brain Morphometry of Portable Low-Field-Strength MRI Using Super-Resolution Machine Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Radiology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{306}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="billot_robust_2022" class="col-sm-8"> <div class="title">Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining</div> <div class="author"> <em>Benjamin Billot</em>, Magdamo Colin, Steven E Arnold, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sudeshna Das, Juan Eugenio Iglesias' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In MICCAI</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-16443-9_52" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2203.01969.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Retrospective analysis of brain MRI scans acquired in the clinic has the potential to enable neuroimaging studies with sample sizes much larger than those found in research datasets. However, analysing such clinical images “in the wild” is challenging, since subjects are scanned with highly variable protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent advances in convolutional neural networks (CNNs) and domain randomisation for image segmentation, best represented by the publicly available method SynthSeg, may enable morphometry of clinical MRI at scale. In this work, we first evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000 scans acquired at Massachusetts General Hospital. We show that SynthSeg is generally robust, but frequently falters on scans with low signal-to-noise ratio or poor tissue contrast. Next, we propose SynthSeg, a novel method that greatly mitigates these problems using a hierarchy of conditional segmentation and denoising CNNs. We show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and state-of-the-art segmentation denoising methods. Finally, we apply our approach to a proof-of-concept volumetric study of ageing, where it closely replicates atrophy patterns observed in research studies conducted on high-quality, 1 mm, T1-weighted scans. The code and trained model are publicly available at https://github.com/BBillot/SynthSeg.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">billot_robust_2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust {Segmentation} of {Brain} {MRI} in the {Wild} with {Hierarchical} {CNNs} and no {Retraining}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Colin, Magdamo and Arnold, Steven E and Das, Sudeshna and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{538--548}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="hoffmann_learning_2022" class="col-sm-8"> <div class="title">SynthMorph: Learning Contrast-Invariant Registration Without Acquired Images</div> <div class="author"> Malte Hoffmann, <em>Benjamin Billot</em>, Douglas N. Greve, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Juan Eugenio Iglesias, Bruce Fischl, Adrian V. Dalca' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Medical Imaging</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9552865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9552865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a strategy for learning image registration without acquired imaging data, producing powerful networks agnostic to contrast introduced by magnetic resonance imaging (MRI). While classical registration methods accurately estimate the spatial correspondence between images, they solve an optimization problem for every new image pair. Learning-based techniques are fast at test time but limited to registering images with contrasts and geometric content similar to those seen during training. We propose to remove this dependency on training data by leveraging a generative strategy for diverse synthetic label maps and images that exposes networks to a wide range of variability, forcing them to learn more invariant features. This approach results in powerful networks that accurately generalize to a broad array of MRI contrasts. We present extensive experiments with a focus on 3D neuroimaging, showing that this strategy enables robust and accurate registration of arbitrary MRI contrasts even if the target contrast is not seen by the networks during training. We demonstrate registration accuracy surpassing the state of the art both within and across contrasts, using a single model. Critically, training on arbitrary shapes synthesized from noise distributions results in competitive performance, removing the dependency on acquired data of any kind. Additionally, since anatomical label maps are often available for the anatomy of interest, we show that synthesizing images from these dramatically boosts performance, while still avoiding the need for real intensity images. Our code is available at doic https://w3id.org/synthmorph.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hoffmann_learning_2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hoffmann, Malte and Billot, Benjamin and Greve, Douglas N. and Iglesias, Juan Eugenio and Fischl, Bruce and Dalca, Adrian V.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Medical Imaging}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SynthMorph: Learning Contrast-Invariant Registration Without Acquired Images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{543-558}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="shapiro_in_vivo_2022" class="col-sm-8"> <div class="title">In vivo hypothalamic regional volumetry across the frontotemporal dementia spectrum</div> <div class="author"> Noah Shapiro, Emily Todd, <em>Benjamin Billot</em>, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'David Cash, Juan Eugenio Iglesias, Warren Jason, Jonathan Rohrer, Martina Bocchetta' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>NeuroImage</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S2213158222001498" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://pdf.sciencedirectassets.com/282794/1-s2.0-S2213158222X0003X/1-s2.0-S2213158222001498/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEL3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICH2CHRhSqklhFUw3o9JD%2B6M%2BtE0nh1J2xmRDGgjKZB7AiEA9uEqw7yQeZ9Xqeh75e6Um6tsMvGgkc%2BzXxpDz9lz5%2FUqvAUI5f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDAV1VQ4d%2FrFqId2nfCqQBSFGfAPpwySoa6mql2H8TbrVK1vrc7CvzUF21V3imOOMUlB56GIgTGdJY%2Fwz%2B6afa%2FO7WxzkBsoUHqNDrF58Bvo07YwRrkPGfqlNWKxmjxMD9yGF47SoPx6YR2LI18WxylLaAgrd7hl%2FVPJsl%2FyOYEOeSmZXf7Ye4sjWjXHmShFcC9uiEyDfVqkB9J1BhG9LCFYHs0RofcUDF0nMTDjqfOGjSDcSMzJqNTMobDidCVUpy7Xgx0s8bN1xm8sz8E8sFC7gEqmr%2BnJpSrPTBk80O1X3j7G95HOk%2BMolbwXlZI7WYgiyNYM11UdYw%2FcFQgFmBYCZLnVAHlaUrELDUgbR0Z98LEeTBdZ7e5RFCoSlkDZD7Jr1iA%2ByLelgpekXncROPM3hbdh7rspmwwZB8I7sB4Hg92v6MIn2aYajoyhUTvl6sFDN%2F0oU6S32mHELyUPoqV062j%2F58QEJ8%2BS0ttsdVfwHC2sLmD450AkJ0rBRted%2B17Xnmi8QllnZ9iX2Mmo6awXKmYCPnAFNSpv19Shi2OfrPoZee5NV21Sp%2FYVRJPHw3s0Ij5GSymeNDJIG1cau9wJ5iAus3QN2dDSbHjXQ7WHKuXQIPG1UNdFzDnGzR9tCuHrbHEBNZy5IqOfEl5f994NVLt0yqJtwOBpcMoTWy54911NCsTlHiJSEs7UrG6l4vmptrJg0m6XelLytPPIyJbElUq2Z24xVocghdKRzPe3XGkJh87Wxtej96FjonOE8q6ZX3kxzYvLeitNBXJMq%2FUQpRvpgEA65R9juSWigo1HLVqXR2Z5g6H7mCrn%2Btj7snY6dJJzadfYkxXxbiep1UZphqDJdIg4w3zRosXJNS58ecB%2BP9GtTZKCYK6mv0WUHMI78gaoGOrEBBgUBd%2BY14zvC8aXnLcq4%2B9XgDouQKwzFXivZr7YmUMwr6o5p2rTZdLEOG0%2FlXBVCMNsl75APt3yK42wUjxXTmfcpmd%2FMh9iNp2xokxXlZmT%2FWG6p5aKVlxOHpIB5hYbhUtCC7pBkNcbCnwXHZF%2FPgVwJIyBqF20vBloPkO%2F63OkidwVimztDuIKISyXR%2B0kmojQuFN2lmZdcZy4gbbVI7ZKtUsnksslgNlb1mGajt12L&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231031T043353Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYVYLOOJ5A%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=96b00868bd217029239d7b84393890ef96b7ba9472ac6b75fd552363f2b7c4d6&amp;hash=4cc9e0092b213c9bc65e08c467d8fd428d1d4dbb24f3a5e91822b2e3c6eea1a0&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S2213158222001498&amp;tid=spdf-22a2ffd4-5201-42d0-82f4-439a07d6aee6&amp;sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f165c520a5950020c&amp;rr=81e92eb35e3b305d&amp;cc=us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Frontotemporal dementia (FTD) is a spectrum of diseases characterised by language, behavioural and motor symptoms. Among the different subcortical regions implicated in the FTD symptomatology, the hypothalamus regulates various bodily functions, including eating behaviours which are commonly present across the FTD spectrum. The pattern of specific hypothalamic involvement across the clinical, pathological, and genetic forms of FTD has yet to be fully investigated, and its possible associations with abnormal eating behaviours have yet to be fully explored.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">shapiro_in_vivo_2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shapiro, Noah and Todd, Emily and Billot, Benjamin and Cash, David and Iglesias, Juan Eugenio and Jason, Warren and Rohrer, Jonathan and Bocchetta, Martina}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeuroImage}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In vivo hypothalamic regional volumetry across the frontotemporal dementia spectrum}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="laguna_super_2022" class="col-sm-8"> <div class="title">Super-resolution of portable low-field MRI in real scenarios: integration with denoising and domain adaptation</div> <div class="author"> Sonia Laguna, Riana Schleicher, <em>Benjamin Billot</em>, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Pamela Schaefer, Brenna McKaig, Joshua Goldstein, Kevin Sheth, Matthew Rosen, Taylor Kimberly, Juan Eugenio Iglesias' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In MIDL</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=pinw0Gcot4T" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=pinw0Gcot4T" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Portable low-field MRI has the potential to revolutionize neuroimaging, by enabling point-of-care imaging and affordable scanning in underserved areas. The lower resolution and signal-to-noise ratio of these scans preclude image analysis with existing tools. Super-resolution (SR) methods can overcome this limitation, but: (i) training with downsampled high-field scans fails to generalize; and (ii) training with paired low/high-field data is hard due to the lack of perfectly aligned images. Here, we present an architecture that combines denoising, SR and domain adaptation modules to tackle this problem. The denoising and SR components are pretrained in a supervised fashion with large amounts of existing high-resolution data, whereas unsupervised learning is used for domain adaptation and end-to-end finetuning. We present preliminary results on a dataset of 11 low-field scans. The results show that our method enables segmentation with existing tools, which yield ROI volumes that correlate strongly with those derived from high-field scans (ρ &gt; 0.8).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">laguna_super_2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Super-resolution of portable low-field MRI in real scenarios: integration with denoising and domain adaptation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Laguna, Sonia and Schleicher, Riana and Billot, Benjamin and Schaefer, Pamela and McKaig, Brenna and Goldstein, Joshua and Sheth, Kevin and Rosen, Matthew and Kimberly, Taylor and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MIDL}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="billot_joint_2021" class="col-sm-8"> <div class="title">Joint Segmentation Of Multiple Sclerosis Lesions And Brain Anatomy In MRI Scans Of Any Contrast And Resolution With CNNs</div> <div class="author"> <em>Benjamin Billot</em>, Stefano Cerri, Koen Van Leemput, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Adrian Dalca, Juan Eugenio Iglesias' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In ISBI</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9434127" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9434127" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present the first deep learning method to segment Multiple Sclerosis lesions and brain structures from MRI scans of any (possibly multimodal) contrast and resolution. Our method only requires segmentations to be trained (no images), as it leverages the generative model of Bayesian segmentation to generate synthetic scans with simulated lesions, which are then used to train a CNN. Our method can be retrained to segment at any resolution by adjusting the amount of synthesised partial volume. By construction, the synthetic scans are perfectly aligned with their labels, which enables training with noisy labels obtained with automatic methods. The training data are generated on the fly, and aggressive augmentation (including artefacts) is applied for improved generalisation. We demonstrate our method on two public datasets, comparing it with a state-of-the-art Bayesian approach implemented in FreeSurfer, and dataset specific CNNs trained on real data. The code is available at https://github.com/BBillot/SynthSeg.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">billot_joint_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joint {Segmentation} {Of} {Multiple} {Sclerosis} {Lesions} {And} {Brain} {Anatomy} {In} {MRI} {Scans} {Of} {Any} {Contrast} {And} {Resolution} {With} {CNNs}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ISBI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Cerri, Stefano and Van Leemput, Koen and Dalca, Adrian and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1971--1974}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="iglesias_joint_2021" class="col-sm-8"> <div class="title">Joint super-resolution and synthesis of 1mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</div> <div class="author"> Juan Eugenio Iglesias, <em>Benjamin Billot</em>, Yaël Balbastre, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Azadeh Tabari, John Conklin, R. Gilberto González, Daniel C. Alexander, Polina Golland, Brian L. Edlow, Bruce Fischl' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>NeuroImage</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811921004833" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811921X00109/1-s2.0-S1053811921004833/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCurdpBWKmb9Vvz3Con4IxCSFg1dIqcK3PnXwNTZczxggIhAI7MjW0YiadYnFYG%2B26ycu2oorEQ%2BY3%2FoiaW3f3DlzIbKroFCOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igwc4KoXF0GCEkoCU7IqjgV5IWiU61oKEdQrVBJOwRhrtfj6okHU%2FmKWeg1lS3XMWUAjDv2kXYm75bWWlTsChK%2F7CvqUirj4qcoxCNRwU91lkBjT4BTNZ%2BfuMpu2zmfZs9yW1hkzVCSJ0co%2Fafg4LY9wP2%2FKsJ51Ns0R2R7sSnNkgj0BNJHwaby4BHCe9iQgiU%2BHWdYXpKnEut7VpxnJL%2FSvgXHUFaofKiYal0sdisK%2BS7FOwP9xU9O8EXTq4R0wwZpwrD3XQebsfOU9ZMSh5Zp18hldjV1wylQnYV75tjE49wZ0Hzmep7lqwqJe%2B1bGmXHbBzcVEp4NtEgz83flezaQ5jUQImAC1tpsmLa%2Bdyp6vJrEUQ9Xqkr1ZM8umIvI%2FXcKL23eeD1oZ4lGbgTLGjAMwWeyRpu9%2Fep6Brvd7I%2BeRspSkzCwCPGA%2FN%2Fqs9NgW1jzwFi7V4A5sBW0eGYBhZhRH4P1%2BIAtRrK6ReRyqE%2Fgays0XWWDRKC8evx7%2FxK4pktTcELTAHMC%2BDvQ2EfHgMH8TuZ2WWsOYBOPTbVa6sYDyLFk5V1Knx%2B2avR8j3D7Mvr7FzwQ1W68lx1VmhPbn%2FfmSr50VxqcGh2N87WwiAEm%2FDQ98r8bUEdEsTmV383QVV7PpwIi0SAZzTFbLzxBBq8XxR9KcGLczwOB%2FNiRMRkGn2vjrnSZ9CZCoG1%2BDe3TXIH43wkVSTXHOOuPbXxXshBBoqYPcv7u5Z0WJt4el0soO2I5A8urHH6fqgdOnGCmorS9G7%2BG7EfRCuhMWk2%2F6H2ARrTZqGAVhF5N1LAp83FV%2FoDlWJAPglbO1KFSEv%2Foa3DxZUKq%2BYki1WiBMuxAkWpmc%2BDMrtkIqxpMrtMR%2BOox8IuTdLmNjAo2DNFBPEYwp8eBqgY6sAH3%2Be%2FNd7sairW9Rb6hTQiCPuMzL%2BsHe26bp09p3NPyd22rFfo4syiBgmJA62wg14wuIwetphkj79HWqLxB3mX76OCdCcizDjmzcGh6cDXzk2o0TeAbZfXrlJtyDMbtvxhUvW5ShtIRGtUe17CwPdRFZdqJO3dhd6uqkLf3jqu0vYz3YZ6LrjqrIuZrt5yS9iKuHDrb6NdilL6ixI5CAVkveEkF0BJt%2F%2Fmj2tarHl1JPA%3D%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231031T030041Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY4K7ONGJH%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=3d536b3e2c078954dc51a4c8f30e8ce1c6bf88aacbde4725ec1741d360ac9361&amp;hash=a0b559d36419a6dbcc8a8288a407afbd641e13862975e12eb3b6e810dbf639fd&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1053811921004833&amp;tid=spdf-45bdd306-d811-4ec3-9007-d63ad0f71b0f&amp;sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f165c520a5803515c&amp;rr=81e8a62c0b143ba5&amp;cc=us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well-typically requiring T1-weighted images (e.g., MP-RAGE scans). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing in clinical settings every year. In turn, the inability to quantitatively analyze these scans hinders the adoption of quantitative neuro imaging in healthcare, and also precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in convolutional neural networks (CNNs) are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the specific combination of contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols – even within sites. In this article, we present SynthSR, a method to train a CNN that receives one or more scans with spaced slices, acquired with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, beyond rigid coregistration of the input scans. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution real images of the input contrasts. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at https://github.com/BBillot/SynthSR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">iglesias_joint_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joint super-resolution and synthesis of 1mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeuroImage}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{237}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Iglesias, Juan Eugenio and Billot, Benjamin and Balbastre, Yaël and Tabari, Azadeh and Conklin, John and {Gilberto González}, R. and Alexander, Daniel C. and Golland, Polina and Edlow, Brian L. and Fischl, Bruce}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="greve_deep_2021" class="col-sm-8"> <div class="title">A deep learning toolbox for automatic segmentation of subcortical limbic structures from MRI images</div> <div class="author"> Douglas N. Greve, <em>Benjamin Billot</em>, Devani Cordero, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Andrew Hoopes, Malte Hoffmann, Adrian Dalca, Bruce Fischl, Juan Eugenio Iglesias, Jean Augustinack' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>NeuroImage</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811921008831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811921X00171/1-s2.0-S1053811921008831/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDUU6FgpVGGKLMbPkSucg%2B3HL3RtE%2FpiRMODJZdD4KmHAIhAJ%2F4Iy57IJRJ0IpDJabD2%2FSVa2GbsyYa0KgnhgkUDfJuKrsFCOT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igw7V0oDxdSsw%2F%2FFMoQqjwW51KJc7tCrVZ74Jqal1%2BjNEj0EDvvIuP1iwks6wavy7%2Fc5J1bF7ZJaG5AibBfIz0ITVDpe4IKEyqt%2Fh63t8ATWNOVLJ6zA%2BkukRfP8wbcLSorVmVl%2Fo9kK7kiliJfUKMczY5YbpuEZo0Vha3cfgIHzBzOXTTWICZboks1OLXlpsb5tGJyzEzJ1xHla70SVV%2BieLOrMoFcBt%2FsEffWfwycEBrhxtfks3D1BiA9QH%2B7oyLkVnzuDkNm3kh0QkdbNQoKBLWYMfwawuw%2BLEbrSAbBpnNgKOG13YLqbEq4lIhXHX2ZgtjD6jnQDMuHozL2WULrz9l6yTuT%2Fx6etoK%2FKqXQKyxqurt%2FbT7Br221RP3riZGv75xu%2Fkn8i9G4CJOdeTkmFVYGAj7YbvZeS5QucPaY6EnXc%2FKJih6KXnQOMZc6YQoqZleNBvpEswBMrf%2FgI3FrdbFQU3%2FoQFa3axTx%2F6u2i%2FbGgpxJ4nP0wsE4NVj9MEZyJ8vrBkf1inTbfGZVP10inlwD8IA8IhG1wpVhHzGai4tYwn%2BZ6uwcsIjy%2FKWDSUM%2FjGVB2jw3WiT%2FDYfcaBbDK7qTg6GhBDMGaGjVGwiaeYDSX9p0XD%2FlaRJnk9YISF4MVdP9pAixLLTBocbpiKC4kVXnwzG1%2Bb8cvhPsB4id8QQEoaFhw7%2BrPVk6hC34dEJQ9gwEaZFJAmsIJx%2FxzxZ2K5S1hB7Bpplvtz2V20zIWEt93a5AF8V5CB8%2BCpiPCbBiZRGI66F%2FwKFWYWsC0Na3QtgR3PMd09SMDFrikDEDxCUQz60wfAo0ZgLha0RyCwsyOBTVj8qzmAvzTzRIxQeGHdRnL3sgWs%2BigojxjL%2FTnZhagvdW43W7QvsC3DOM4MNDkgaoGOrAB6rLeuLN3wkxLlPUiBY6C6njs8H1NSTYRPkVP9qgte%2FdyjIOJnoeUNW2IOgX5Q8F5qhLhB4P5VQLvQUu7%2FbwOJzZjNJiApBE1m1Og2qg7oiUtOoEdKaA1kM0x2%2BzpIny3iXAslZcEw6Fxync%2BmDz%2FRt%2BH6DKyuK4X1l4ngFyISffUWXH922HlGDyFhdKMM9HJB0WjyIfBkSkYJvykdGamorBwLtGL5%2BecaVVyeduzWbo%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231031T042233Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYYMII4FWV%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=eae8655680b99d7528978ce2691d7fc8add4c3c05ad9a2ad15090734e14b45c6&amp;hash=e99b26223746ccf492456584a05e4ec9a658e9e5f5293ed60fb9ab54484e6be5&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1053811921008831&amp;tid=spdf-7cf7382a-35f3-4d63-a3c5-c5c8a090081d&amp;sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f165c520a5953025f&amp;rr=81e91e1bfc334cd0&amp;cc=us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>A tool was developed to automatically segment several subcortical limbic structures (nucleus accumbens, basal forebrain, septal nuclei, hypothalamus without mammillary bodies, the mammillary bodies, and fornix) using only a T1-weighted MRI as input. This tool fills an unmet need as there are few, if any, publicly available tools to segment these clinically relevant structures. A U-Net with spatial, intensity, contrast, and noise augmentation was trained using 39 manually labeled MRI data sets. In general, the Dice scores, true positive rates, false discovery rates, and manual-automatic volume correlation were very good relative to comparable tools for other structures. A diverse data set of 698 subjects were segmented using the tool; evaluation of the resulting labelings showed that the tool failed in less than 1% of cases. Test-retest reliability of the tool was excellent. The automatically segmented volume of all structures except mammillary bodies showed effectiveness at detecting either clinical AD effects, age effects, or both. This tool will be publicly released with FreeSurfer (surfer.nmr.mgh.harvard.edu/fswiki/ScLimbic). Together with the other cortical and subcortical limbic segmentations, this tool will allow FreeSurfer to provide a comprehensive view of the limbic system in an automated way.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">greve_deep_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A deep learning toolbox for automatic segmentation of subcortical limbic structures from MRI images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Greve, Douglas N. and Billot, Benjamin and Cordero, Devani and Hoopes, Andrew and Hoffmann, Malte and Dalca, Adrian and Fischl, Bruce and Iglesias, Juan Eugenio and Augustinack, Jean}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeuroImage}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{244}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="billot_partial_2020" class="col-sm-8"> <div class="title">Partial Volume Segmentation of Brain MRI Scans of Any Resolution and Contrast</div> <div class="author"> <em>Benjamin Billot</em>, Eleanor Robinson, Adrian V. Dalca, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Juan Eugenio Iglesias' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In MICCAI</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-59728-3_18" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2004.10221.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Partial voluming (PV) is arguably the last crucial unsolved problem in Bayesian segmentation of brain MRI with probabilistic atlases. PV occurs when voxels contain multiple tissue classes, giving rise to image intensities that may not be representative of any one of the underlying classes. PV is particularly problematic for segmentation when there is a large resolution gap between the atlas and the test scan, e.g., when segmenting clinical scans with thick slices, or when using a high-resolution atlas. Forward models of PV are realistic and simple, as they amount to blurring and subsampling a high resolution (HR) volume into a lower resolution (LR) scan. Unfortunately, segmentation as Bayesian inference quickly becomes intractable when “inverting” this forward PV model, as it requires marginalizing over all possible anatomical configurations of the HR volume. In this work, we present PV-SynthSeg, a convolutional neural network (CNN) that tackles this problem by directly learning a mapping between (possibly multi-modal) LR scans and underlying HR segmentations. PV-SynthSeg simulates LR images from HR label maps with a generative model of PV, and can be trained to segment scans of any desired target contrast and resolution, even for previously unseen modalities where neither images nor segmentations are available at training. PV-SynthSeg does not require any preprocessing, and runs in seconds. We demonstrate the accuracy and flexibility of our method with extensive experiments on three datasets and 2,680 scans. The code is available at https://github.com/BBillot/SynthSeg.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">billot_partial_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Partial {Volume} {Segmentation} of {Brain} {MRI} {Scans} of {Any} {Resolution} and {Contrast}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Robinson, Eleanor and Dalca, Adrian V. and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{177--187}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EAADDA"><a href="">Conf.</a></abbr></div> <div id="billot_learning_2020" class="col-sm-8"> <div class="title">A Learning Strategy for Contrast-agnostic MRI Segmentation</div> <div class="author"> <em>Benjamin Billot</em>, Douglas N. Greve, Koen Van Leemput, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Bruce Fischl, Juan Eugenio Iglesias, Adrian Dalca' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In MIDL</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v121/billot20a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.mlr.press/v121/billot20a/billot20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present a deep learning strategy for contrast-agnostic semantic segmentation of unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources. In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic scans of widely varying contrasts on the fly during training. These scans are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any specific MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four MR contrasts. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at: https://github.com/BBillot/SynthSeg.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">billot_learning_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A {Learning} {Strategy} for {Contrast}-agnostic {MRI} {Segmentation}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MIDL}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Greve, Douglas N. and Leemput, Koen Van and Fischl, Bruce and Iglesias, Juan Eugenio and Dalca, Adrian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{75--93}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DB50B7"><a href="">Journal</a></abbr></div> <div id="billot_automated_2020" class="col-sm-8"> <div class="title">Automated segmentation of the hypothalamus and associated subunits in brain MRI</div> <div class="author"> <em>Benjamin Billot</em>, Martina Bocchetta, Emily Todd, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Adrian V. Dalca, Jonathan D. Rohrer, Juan Eugenio Iglesias' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>NeuroImage</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811920307734" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811920X00152/1-s2.0-S1053811920307734/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDbQ1bOpbB5Hi0qLKePBehucNkzSfcivIN84gu33JeJ7AiAApU8H8aQzDMazpvTStc9o2u4lmet9XbNYXlBHx%2B4QZSq7BQjj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMtnJ0y1JSIftJ5V%2FOKo8FnRJNTqDOkFymNEUBTqIY%2FmN%2F5z79UWrIvJKOVbV6XbJmMc%2FLO3RrfFWS2cvo2DVKWYm1DecxkEFmD05VEcbyDESSkjh5Pfuob3uw0%2Fyu7c32553q378rZTcA%2BKZWvZOPMderOuJggBLKkgzjVacV5T5CdxLNOhK%2FL3BG7RcuMhtUWpkK7g6Cis4bywGQuKX%2Bk9Mt8Rb%2Bps6ekAkI%2BKKVn79AGw1gqP6CA3%2BZTqUZXBqx76eUIEFmOvcRNwznRpiUdIbaaPNIaDNoZj5W6i2HipSF0JJ8yhAWNo8%2Fvmh6Z1rL9LhGdqp6EsvsB38qZXf0J7DFpydfNM%2FKZfhzgtMgleukwaYmzm8MkcVMzTNgfmrBgFqgY4u0nQVxL9TbBIXRlLd0fEcDGMh03Ho5p2GRtCWClnp%2B%2F%2FdcmJ7VhrCbVDJ2LFpWT%2F3ttBYj5gnXYUd7B1Q5dwlweB36hiIj9gGWx%2F4FQl9PAOwMMjCLQcx5ec2T1Vr%2B03wTnDF9rBoiJXdc3X0yG2qc8QLapZLlziPw5Rk4Pw7RCNUKPyH3iVvucuVMFVBWYKavCmCjf69lcoB0wNsKLj3FrzkEkICM97BhJzMYOodOYDyHp9wszr8kIhLer3OTwIFko9VN3YnmIPcgKSyyxf12zZdtfc4NCm%2FzUU8gXSPQdhjLrijFrr1wxJ9WL1dhfi3T1rbhmtIIulhaZ14aTF5eo8pFRMzTmwoJOIcGVz1IeY37QC%2FHVVH3BEKXOV5dr7lenHDNucXoqXJf1I%2FLTvyHXjk0AS4fw0x9jnRL%2Blafk10NABxu5RhFP3EchTDoElPwli8VEM12hUPCVLBtVmlZHwCj6iwSpS%2BpyMMb8IhgGYLJICKhJuv%2FOjDltoGqBjqyAQcJZkcyYwzEYy16YkUqD4wenaLMB02OClXvN5DH6eWgB7OgRipu%2BE%2B2bP3d862SaRS1WSzByywTn%2Fln5b0lPGF4Bn%2BaeJtIqA8ffLdsN9unoeoAK6VOwGpeepsOSAmY3RQDntZXs7nQzIXVM3fU5yo%2BJykOv%2BfkMxRs0anZxGiJEYO5RR8NiWdr8L8cTtuyWXQ5Z5cT7%2F5dw0v%2FqUUoXhbSYLA50NvTaCr3hGQTe9S8mBQ%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231031T024317Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYXDXUNUXW%2F20231031%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=a32797cbde3a7632c9068523aadc6c71f5d5185a83abe68308666bc660be41fe&amp;hash=0445c59e26c3da9daba534780b03411e9f83b475ca74eeca87cdd405248cb157&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1053811920307734&amp;tid=spdf-bd20a4d3-504c-422a-9265-3bf6b5d22771&amp;sid=fbe70a1278b31342850b9056ddf7331b38eagxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f165c520a585a040f&amp;rr=81e88caf886f4cf9&amp;cc=us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Despite the crucial role of the hypothalamus in the regulation of the human body, neuroimaging studies of this structure and its nuclei are scarce. Such scarcity partially stems from the lack of automated segmentation tools, since manual delineation suffers from scalability and reproducibility issues. Due to the small size of the hypothalamus and the lack of image contrast in its vicinity, automated segmentation is difficult and has been long neglected by widespread neuroimaging packages like FreeSurfer or FSL. Nonetheless, recent advances in deep machine learning are enabling us to tackle difficult segmentation problems with high accuracy. In this paper we present a fully automated tool based on a deep convolutional neural network, for the segmentation of the whole hypothalamus and its subregions from T1-weighted MRI scans. We use aggressive data augmentation in order to make the model robust to T1-weighted MR scans from a wide array of different sources, without any need for preprocessing. We rigorously assess the performance of the presented tool through extensive analyses, including: inter- and intra-rater variability experiments between human observers; comparison of our tool with manual segmentation; comparison with an automated method based on multi-atlas segmentation; assessment of robustness by quality control analysis of a larger, heterogeneous dataset (ADNI); and indirect evaluation with a volumetric study performed on ADNI. The presented model outperforms multi-atlas segmentation scores as well as inter-rater accuracy level, and approaches intra-rater precision. Our method does not require any preprocessing and runs in less than a second on a GPU, and approximately 10 seconds on a CPU. The source code as well as the trained model are publicly available at https://github.com/BBillot/hypothalamus_seg, and will also be distributed with FreeSurfer.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">billot_automated_2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated segmentation of the hypothalamus and associated subunits in brain {MRI}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{223}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeuroImage}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Billot, Benjamin and Bocchetta, Martina and Todd, Emily and Dalca, Adrian V. and Rohrer, Jonathan D. and Iglesias, Juan Eugenio}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Benjamin Billot. Last updated: November 02, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>